Core Concepts & Guidance
  • Delivery Metrics are used to track contract status against delivery plans and outcomes, focusing on effort, schedule, and quality.
  • Delivery Leads must measure, report, review, and act on metrics across financial, operational, and delivery areas for a balanced scorecard.
  • Metrics provide early warning of delivery issues, enabling proactive risk mitigation and improved client satisfaction.
  • Quality of reported data is critical—metrics are for management, not just compliance.
  • Metrics should drive continuous improvement, not just show “all green” status.
  • Use judgment and client/commercial context to select additional metrics beyond the core set.
  • Policy 11 mandates use of Accenture Delivery Methods (ADM), including metrics, for all work types.

Core Technology Delivery Metrics (by Work Type)
System Integration (SI) Waterfall
  • Cost Performance Index (CPI) – Effort & Cost
  • Schedule Performance Index (SPI)
  • Variance At Completion (VAC) – Effort & Cost
  • End Date Variance (EDV)
  • Requirements Volatility (RV)
  • Cost of Rework (COR)
  • Delivered Defect Rate
  • Unit Test Code Coverage %
System Integration (SI) Agile
  • Agile Release Backlog Volatility (ARBV)
  • Release Slippage Risk Indicator – Velocity (RSRI-V)
  • Story Point Effort Performance Indicator (SPEPI)
  • Release Burn Chart Performance Indicator (RBPI)
  • Process AdherenceRelease (PAR)
  • Running Tested Features – Release (RTFR)
  • Delivered Product Quality Index (DPQI)
  • Delivered Defect Rate
  • Release Predictability
  • Release Burn
  • Defect Density
  • Normalized Velocity (Productivity)
  • Unit Test Code Coverage %
  • Average User Story Cycle Time
  • Deployment Frequency
Testing
  • Defect Rate (DR)
  • Defect Removal Efficiency (DRE)
  • Defect Closure Trend (DCT)
  • Test Case Pass Trend (TCPT)
  • Test Artefact Development Trend (TADT)
  • Test Artefact Execution Trend (TAET)
  • Test Automation %
  • Test Case Execution Trend (TCET)
Application Maintenance
  • Percentage SLAs Met (SLAM)
  • Average Resolution Effort – Incidents (ARE-I)
  • Average Resolution Effort – Problems (ARE-P)
  • Backlog Processing Efficiency – Incidents (BPE-I)
  • Backlog Processing Efficiency – Problems (BPE-P)
  • Ageing (AGE)
  • Delivered Defects (DD)
Infrastructure Services (IO Service Desk & Delivery)
  • Overall Customer Satisfaction (CSAT)
  • Percentage of Voice Answered within SLA (VSLA)
  • Average Handle Time – Voice (AHT-V)
  • Voice Abandoned Rate (VAR)
  • Closed on Initial % (First Call Resolution, COI)
  • Number of Major Incidents Reported (MIR)
  • Change & Release Success % (C&RS)
  • Percentage SLAs Met (SLAM)
Capacity Services
  • % of Actual Onboard vs Planned (AOvP)
  • Resource Quality (recommended if resource rejection rates are a problem)

Metric Setup & Reporting
  • Metrics are configured and reported in myWizard, with options for custom metrics and thresholds.
  • Metrics can be reported at E2E (end-to-end) and team levels, and aggregated as needed.
  • Thresholds (Green/Amber/Red) can be customized to drive continuous improvement.
  • Submission and reporting guidance is provided for common challenges and best practices.

Continuous Improvement & Corrective Actions
  • Metrics should be used to identify root causes and drive corrective actions, such as re-planning, process improvements, training, and resource adjustments.
Regular review and analysis of metrics helps prevent client escalations and negative financial variances.